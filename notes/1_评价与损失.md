# 评价与损失

## 评价函数

在分类问题中，**评价函数**(Score function)将样本的特征映射为各个类别的评分，然后依据每个类别评分的高低确定该样本的类别$$f:R^D\xrightarrow{W} R^K$$其中$D$是特征的维数，$K$是类别的个数，$W$是评价函数的权重系数

## 损失函数

### 概念

1. 损失函数用来衡量模型基于某个权重$W$的预测和真实值之间的拟合程度

2. 损失函数由数据损失和正则化损失构成$$\begin{aligned}L&=L_{data}+L_{reg}\\&=\frac{1}{N}\sum_iL_i+\lambda R(W)\end{aligned}$$

    * 数据损失评估了模型预测值与真实值的差异

    * 正则化损失评估了模型的复杂度

### 常见数据损失函数

1. 在分类问题中，每个样本都有一个正确的标签

    * 多分类SVM：**折页损失函数**(Hinge loss function)$$L_i=\sum_{j\neq y_i}\max(0, f_j+f_{y_i}+\Delta)$$

    * Softmax分类：**交叉熵损失函数**(Cross-entropy loss function)$$L_i=-\log\left(\frac{e^{f_{y_i}}}{\sum_je^{f_j}}\right)$$

2. 在属性分类问题中，每个样本都有一个正确的标签向量$y$，$y_i$取值为1或0，代表有或没有这个属性，通常是为每个属性训练一个二分类器

    * 二分类SVM：**折页损失函数**$$L_i=\sum_j\max(0,1-y_{ij}f_j)$$

    * 逻辑回归：**对数损失函数**$$L_i=\sum_j\left[y_{ij}\log(\sigma(f_j))+(1-y_{ij})\log(1-\sigma(f_j))\right]$$其中$\sigma$是$\text{sigmoid}$函数

3. 在回归问题中，需要拟合每一个样本

    * **L2平方范式**$$L_i=||f-y_i||_2^2$$

    * **L1范式**$$\begin{aligned}L_i&=||f-y_i||_1\\&=\sum_j|f_j-y_{ij}|\end{aligned}$$

    回归问题中的L2损失相比Softmax交叉熵损失更难优化，并且它的鲁棒性不好，一个异常值会导致很大的梯度

    因此面对一个回归问题，先考虑是不是必须这样做，一般而言，可以对它们进行分类，变成一个分类问题

# 邻近分类器

## NN分类器

1. **邻近分类器**(Nearest Neighbor classifier)会将测试样本与每一个训练样本进行比较，然后将它认为最相似的训练样本的标签赋予测试样本

2. 通常用$L_1$距离来衡量两个样本间的相似度$$d_1(I_1, I_2)=\sum_i|I_1-I_2|$$也可以使用$L_2$距离$$d_2(I_1,I_2)=\sqrt{\sum_i(I_1-I_2)^2}$$

3. 无论是使用$L_1$距离还是使用$L_2$距离的NN分类器都在图像分类上表现不佳

## K-NN分类器

1. 相比NN分类器，**k-NN分类器**(K-Nearest Neighbor classifier)寻找最相似的$k$个标签，然后对测试样本进行投票，把票数最高的标签作为对测试样本的预测

2. 在图像分类上，越高的$k$值可以让决策边界更加平滑，泛化能力更好，如下图

    <div align="center"><img src="img/knn.jpg" style="height:160px"/></div>

    $k$是一个超参数

## NN分类器在图像分类中的缺点

1. NN分类器训练不需要花费时间，只需要将训练数据存储起来，在测试阶段需要花费大量的时间比较样本之间的差异

    但在实际应用中，测试效率远远重要于训练效率

2. 无论是$L_1$距离还是$L_2$距离都难度量图片间的差异度